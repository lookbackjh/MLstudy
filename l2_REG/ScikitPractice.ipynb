{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Example of Minmax Featurization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin ## inheritance..\n",
    "class Minmaxreg(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,a) -> None:\n",
    "        self.a=a\n",
    "    def fit(self,X):\n",
    "        self.X_max=np.max(X)\n",
    "        self.X_min=np.min(X)\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        \n",
    "        try:\n",
    "            getattr(self, \"X_min\") or getattr(self,\"X_max\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data!\")\n",
    "        X=X.copy()\n",
    "        X=(X-self.X_min)/(self.X_max-self.X_min)\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_reg: [0.         0.11111111 0.22222222 0.33333333 0.44444444 0.55555556\n",
      " 0.66666667 0.77777778 0.88888889 1.        ]\n",
      "X_test_reg: [0.  0.5 1. ]\n"
     ]
    }
   ],
   "source": [
    "X_train=[1,2,3,4,5,6,7,8,9,10]\n",
    "X_test=[1,3,5]\n",
    "temp=4\n",
    "scale=Minmaxreg(temp)\n",
    "scale.fit(X_train)\n",
    "print(\"X_train_reg: {}\".format(scale.transform(X_train))) ## 결과 1\n",
    "\n",
    "scale.fit(X_test)\n",
    "print(\"X_test_reg: {}\".format(scale.transform(X_test))) ## 결과 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_reg: [0.         0.11111111 0.22222222 0.33333333 0.44444444 0.55555556\n",
      " 0.66666667 0.77777778 0.88888889 1.        ]\n",
      "X_test_reg: [0.         0.22222222 0.44444444]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"X_train_reg: {}\".format(scale.fit_transform(X_train))) ## 결과 1\n",
    "print(\"X_test_reg: {}\".format(scale.transform(X_test))) ## 결과 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Minmaxreg(a=1653245)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale.get_params()\n",
    "scale.set_params(**{'a':1653245})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ridge Rigression class\n",
    "\n",
    "아래코드는 본강의 의 HW-2 에있는 ridge_regression.py를 그대로 작성한 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from scipy.optimize import minimize, leastsq\n",
    "from setup_problem import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gradient import *\n",
    "class RidgeRegression(BaseEstimator, RegressorMixin):\n",
    "    \"\"\" ridge regression\"\"\"\n",
    "\n",
    "    def __init__(self, l2reg=1):\n",
    "        if l2reg < 0:\n",
    "            raise ValueError('Regularization penalty should be at least 0.')\n",
    "        self.l2reg = l2reg\n",
    "        self.w_=0\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        n, num_ftrs = X.shape\n",
    "        # convert y to 1-dim array, in case we're given a column vector\n",
    "        y = y.reshape(-1)\n",
    "        def ridge_obj(w):\n",
    "            predictions = np.dot(X,w)\n",
    "            residual = y - predictions\n",
    "            empirical_risk = np.sum(residual**2) / n\n",
    "            l2_norm_squared = np.sum(w**2)\n",
    "            objective = empirical_risk + self.l2reg * l2_norm_squared\n",
    "            return objective\n",
    "        self.ridge_obj_ = ridge_obj\n",
    "\n",
    "        w_0 = np.random.rand(num_ftrs)\n",
    "        self.w_ = minimize(ridge_obj,w_0).x\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        try:\n",
    "            getattr(self, \"w_\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data!\")\n",
    "        return np.dot(X, self.w_)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        # Average square error\n",
    "        try:\n",
    "            getattr(self, \"w_\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data!\")\n",
    "        y = y.reshape(-1)\n",
    "        residuals = self.predict(X) - y\n",
    "        return np.dot(residuals, residuals)/len(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.88611996 3.56771314 3.75686296 3.45612732 3.33523948 3.4524712\n",
      " 3.25370604 3.25374748 3.85940386 3.12527637 3.14969656 3.46280067\n",
      " 3.66479507 3.24839118 3.29621433 3.08322493 3.8973187  3.87058627\n",
      " 3.28568792 3.86812922 3.68765039 3.94750866 3.65141152 3.52469342\n",
      " 3.82699152 3.85078643 3.20908756 3.51396306 3.61286517 3.04863113\n",
      " 3.20328745 3.8458258  2.96595707 3.32467843 3.17910169 3.76505065\n",
      " 3.03738217 3.8650506  3.18467165 3.82363615 3.26044198 3.68788051\n",
      " 3.9382866  3.21256968 3.97520851 3.45123161 3.61641466 3.54726602\n",
      " 3.90809074 3.2077914  3.68273243 3.14675312 3.27072461 3.31915187\n",
      " 3.53459214 3.25024768 3.94204119 3.51524523 3.89919966 2.96100786\n",
      " 3.46022365 3.46334131 3.46552773 3.88932486 3.05532607 3.8382798\n",
      " 3.36692323 3.44178414 3.06620669 3.82367069 3.80888024 3.49907278\n",
      " 3.1686451  3.68592752 3.41158578 3.15324427 2.95972733 3.68099365\n",
      " 3.59133152 3.17308082 3.88546797 3.97002535 3.26510667 3.80953833\n",
      " 2.94736561 3.45244444 3.92657169 3.0346473  3.47281658 3.91523663\n",
      " 3.32329945 3.17580881 3.37052518 3.16766921 3.72742508 3.17958171\n",
      " 3.22950769 3.65250077 3.67998405 2.99402201]\n",
      "0.15730020410395984\n"
     ]
    }
   ],
   "source": [
    "index=np.arange(0,100,1)\n",
    "N = 100\n",
    "theta = np.array([[1], [3]])\n",
    "X = np.c_[np.random.rand(N,1), np.ones((N,1))]\n",
    "y = np.dot(X,theta)+ 0.4*np.random.randn(N,1)\n",
    "ridge=RidgeRegression(l2reg=0.01)\n",
    "ridge.fit(X,y)\n",
    "print(ridge.predict(X))\n",
    "print(ridge.score(X,y))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
