{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt \r\n",
    "from gradient import *\r\n",
    "import pandas\r\n",
    "import csv\r\n",
    "## Data Generation\r\n",
    "n=100\r\n",
    "X=np.hstack([np.ones((n,1)),np.random.rand(n,1)]) ### Design Matrix For X\r\n",
    "##print(X)\r\n",
    "theta=np.array([4,3])\r\n",
    "Y=np.dot(X,theta)+np.random.normal(0,1,n)\r\n",
    "#plt.scatter(X[:,1],Y)\r\n",
    "##scattered image of X and Y.. Now Lets do the optimization\r\n",
    "df=pandas.read_csv(\"data.csv\")\r\n",
    "\r\n",
    "y=df['y']\r\n",
    "X=df.loc[:,df.columns!='y']\r\n",
    "X_train,X_test,y_train,y_test=splitData(X,y)\r\n",
    "X_train=featurescale(X_train)\r\n",
    "X_test=featurescale(X_test)\r\n",
    "theta_init=np.random.rand(X_train.shape[1])\r\n",
    "print(theta_init.shape)\r\n",
    "loss=SquareLossfunction(X_train,y_train,theta_init)\r\n",
    "print(loss)\r\n",
    "num_iter=150\r\n",
    "theta_hists,loss_hists=gradDescent(X_train,y_train,num_iter=num_iter)\r\n",
    "print(loss_hists)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(48,)\n",
      "174.72437661094872\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\junho\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:364: RuntimeWarning: overflow encountered in square\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1.90921090e+002 9.23424575e+006 4.64301951e+011 2.33453446e+016\n",
      " 1.17381612e+021 5.90200878e+025 2.96756084e+030 1.49210510e+035\n",
      " 7.50238237e+039 3.77223705e+044 1.89670049e+049 9.53670916e+053\n",
      " 4.79510719e+058 2.41100494e+063 1.21226588e+068 6.09533617e+072\n",
      " 3.06476687e+077 1.54098079e+082 7.74813192e+086 3.89580121e+091\n",
      " 1.95882920e+096 9.84909554e+100 4.95217669e+105 2.48998031e+110\n",
      " 1.25197511e+115 6.29499624e+119 3.16515700e+124 1.59145747e+129\n",
      " 8.00193131e+133 4.02341286e+138 2.02299301e+143 1.01717145e+148\n",
      " 5.11439119e+152 2.57154261e+157 1.29298506e+162 6.50119642e+166\n",
      " 3.26883552e+171 1.64358758e+176 8.26404419e+180 4.15520459e+185\n",
      " 2.08925857e+190 1.05049013e+195 5.28191923e+199 2.65577658e+204\n",
      " 1.33533834e+209 6.71415092e+213 3.37591016e+218 1.69742526e+223\n",
      " 8.53474290e+227 4.29131332e+232 2.15769475e+237 1.08490019e+242\n",
      " 5.45493485e+246 2.74276974e+251 1.37907895e+256 6.93408102e+260\n",
      " 3.48649216e+265 1.75302647e+270 8.81430867e+274 4.43188045e+279\n",
      " 2.22837264e+284 1.12043740e+289 5.63361780e+293 2.83261246e+298\n",
      " 1.42425234e+303             inf             inf             inf\n",
      "             inf             inf             inf             inf\n",
      "             inf             inf             inf             inf\n",
      "             inf             inf             inf             inf\n",
      "             inf             inf             inf             inf\n",
      "             inf             inf             inf             inf\n",
      "             inf             inf             inf             inf\n",
      "             inf             inf             inf             inf\n",
      "             inf             inf             inf             inf\n",
      "             inf             inf             inf             inf\n",
      "             inf             inf             inf             inf\n",
      "             inf             inf             inf             inf\n",
      "             inf             inf             inf             inf\n",
      "             inf             inf             inf             inf\n",
      "             inf             inf             inf             inf\n",
      "             inf             inf             inf             inf\n",
      "             inf             inf             inf             inf\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\r\n",
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "def featurescale(X_train,X_test):\r\n",
    "    ## before doing the gradient, the data must be feature scaled.., \r\n",
    "    ## feature이 여러가지일때, 특정 feature의 범위는 gradient 를 시행할때, 큰 영향을 줄수있다. i.e 키, 몸무게가 feature 인 경우.\r\n",
    "    ## 따라서, feature의 크기를 어느정도 조절해주는 도구가 필요한데, 이게 feature Scaling이다.\r\n",
    "    ## feature scaling을 함에 있어서, gradient descent의 수렴속도가 훨씬 빨라질 수 잇다. \r\n",
    "    ## feature scaling은 min-max scaling과 standard normal scaling이 있는데, 두가지 모두 자주 사용된다. \r\n",
    "    ## 두가지 방법 은 쓰임세가 살짝 다르긴한데, 대부분의 상황에서 통용될수 있는 min-max scaling을 사용하도록 하겠다.\r\n",
    "    scalar=MinMaxScaler()\r\n",
    "    scalar.fit(X_train)\r\n",
    "    X_train=scalar.transform(X_train)\r\n",
    "    X_test=scalar.transform(X_test)\r\n",
    "    return X_test,X_train\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "def Lossfunction(X,Y,theta, l2_reg=0.01):\r\n",
    "    ## Square Loss \r\n",
    "    ## X: n* d matrix  : n=number of data, d=number of feature\r\n",
    "    ## Y: n*1 matrix: n= number of data\r\n",
    "    ## theta: 1* d matrix, d: number of feature\r\n",
    "    ## loss =avg(X*thata.T-Y+ l2_reg*(L2norm(theta)))\r\n",
    "    loss_term=np.mean(np.dot(X,theta)-Y)\r\n",
    "    reg_term=np.linalg.norm(theta)\r\n",
    "    loss=loss_term+ reg_term*l2_reg\r\n",
    "    return loss\r\n",
    "\r\n",
    "init_theta=np.array([0,0])\r\n",
    "loss=Lossfunction(X,Y,init_theta)\r\n",
    "print(loss)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-5.216880115533555\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "a=np.array(([1,2,3],[1,5,6],[1,1,9]))\r\n",
    "b=np.array(([1,3,4],[4,7,8]))\r\n",
    "scalar=MinMaxScaler()\r\n",
    "scalar.fit(a)\r\n",
    "print(scalar.transform(a))\r\n",
    "#### Scaling 먼저 하고 붙이기\r\n",
    "theta_init=np.random.rand(1,10)\r\n",
    "print(theta_init)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.   0.25 0.  ]\n",
      " [0.   1.   0.5 ]\n",
      " [0.   0.   1.  ]]\n",
      "[[0.19021235 0.76042522 0.79105922 0.36630561 0.13685656 0.16775966\n",
      "  0.94190243 0.34681511 0.78607474 0.37505275]]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit (windows store)"
  },
  "interpreter": {
   "hash": "4ba6b5235c472148a5e4944b493e5c97799165cf05377d2858f35c1701e127b6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}